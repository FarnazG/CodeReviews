{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "\n",
    "def load_GloVe_vectors(file, vocab):\n",
    "    \"\"\"\n",
    "    This function will load Global Vectors for words in vocab from the specified GloVe file.\n",
    "    \n",
    "    INPUT:\n",
    "    file  = The path/filename of the file containing GloVe information.\n",
    "    vocab = The list of words that will be loaded from the GloVe file.\n",
    "    \"\"\"\n",
    "    glove = {}\n",
    "    with open(file, 'rb') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0].decode('utf-8')\n",
    "            if word in vocab:\n",
    "                vector = np.array(parts[1:], dtype=np.float32)\n",
    "                glove[word] = vector\n",
    "    return glove\n",
    "\n",
    "\n",
    "#Creating Mean Word Embeddings using Mean Embedding Vectorizer class\n",
    "class W2vVectorizer(object):\n",
    "    \"\"\"\n",
    "    This class is used to provide mean word vectors for review documents. \n",
    "    This is done in the transform function which is used to generate mean vectors in model pipelines.\n",
    "    The class has both fit and transform functions so that it may be used in an sklearn Pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.w2v = model\n",
    "        \n",
    "        #If using GloVe the model is in a dictionary format\n",
    "        if isinstance(model, dict):\n",
    "            if len(model) == 0:\n",
    "                self.dimensions = 0\n",
    "            else:\n",
    "                self.dimensions = len(model[next(iter(model))])\n",
    "        #Otherwise, using gensim keyed vector\n",
    "        else:\n",
    "            self.dimensions = model.vector_size\n",
    "    \n",
    "    # Need to implement a fit method as required for sklearn Pipeline.\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        This function generates a w2v vector for a set of tokens. This is done by taking \n",
    "        the mean of each token in the review.\n",
    "        \"\"\"\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                    or [np.zeros(self.dimensions)], axis=0) for words in X])\n",
    "    \n",
    "class KerasTokenizer(object):\n",
    "    \"\"\"\n",
    "    This class is used to fit text and convert text to sequences for use in a Keras NN Model.\n",
    "    The class has both fit and transform functions so that it may be used in an sklearn Pipeline.\n",
    "    num_words = max number of words to keep.\n",
    "    maxlen  = max length of all sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_words=20000, maxlen=100):\n",
    "        self.tokenizer = text.Tokenizer(num_words=num_words)\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.tokenizer.fit_on_texts(X)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return sequence.pad_sequences(self.tokenizer.texts_to_sequences(X), maxlen=self.maxlen)\n",
    "    \n",
    "class KerasModel(object):\n",
    "    \"\"\"\n",
    "    This class is used to fit and transform a keras model for use in an sklearn Pipeline.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, epochs=3, batch_size=32, validation_split=0.1):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        \n",
    "    def set_params(self, epochs=3, batch_size=32, validation_split=0.1):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y_dummies = pd.get_dummies(y).values\n",
    "        self.labels = np.array(pd.get_dummies(y).columns)\n",
    "        self.model.fit(X, y_dummies, epochs=self.epochs, batch_size=self.batch_size, validation_split=self.validation_split)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.model.predict(X)\n",
    "        return [self.labels[idx] for idx in y_pred.argmax(axis=1)]\n",
    "    \n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "        \n",
    "    \n",
    "def custom_accuracy(y_true, y_pred, threshold=1, credit_given=0.5):\n",
    "    \"\"\"\n",
    "    If y_pred is off by threshold or less, give partial credit according to credit_given.\n",
    "    INPUTS:\n",
    "    y_true       = True label.\n",
    "    y_pred       = Predicted label.\n",
    "    threshold    = Threshold for giving credit to inaccurate predictions. (=1 gives credit to predictions off by 1)\n",
    "    credit_given = Partial credit amount for close but inaccurate predictions. (=0.5 give 50% credit)\n",
    "    \"\"\"\n",
    "    predicted_correct = sum((y_true-y_pred)==0)\n",
    "    predicted_off = sum(abs(y_true-y_pred)<=threshold) - predicted_correct\n",
    "    custom_accuracy = (predicted_correct + credit_given*predicted_off)/len(y_true)\n",
    "    return custom_accuracy\n",
    "\n",
    "\n",
    "def get_gridsearch_result(name, estimator, param_grid, X_train, X_test, y_train, y_test, cv=4, scoring='accuracy'):\n",
    "    \"\"\"\n",
    "    This function fits a GridSearchCV model and populates a dictionary containing the best model and accuracy results.\n",
    "    \n",
    "    INPUTS:\n",
    "    name       = The name of the gridsearch model desired. It will be used as a key in the returned dictionary object.\n",
    "    estimator  = The model which will be passed into GridSearchCV. It can be a pipeline model or a base model.\n",
    "    param_grid = The parameter grid to be used by GridSearchCV.\n",
    "    X_train, X_test, y_train, y_test = train test split of data(X) and target(y).\n",
    "    cv         = Number of cross validations to perform.\n",
    "    \n",
    "    RETURN:\n",
    "    Dictionary containing the fitted GridSearchCV model as well as summary metrics.\n",
    "    Dictionary keys are: name, model, model params, accuracy train, accuracy test, \n",
    "                         custom accuracy train, custom accuracy test.\n",
    "    \"\"\"\n",
    "    grid_clf = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=cv, scoring=scoring)\n",
    "    grid_clf.fit(X_train,y_train)\n",
    "\n",
    "    result = {}\n",
    "    \n",
    "    result['name'] = name\n",
    "    result['model'] = grid_clf.best_estimator_\n",
    "    result['model params'] = grid_clf.best_params_\n",
    "\n",
    "    y_pred_train = grid_clf.predict(X_train)\n",
    "    y_pred_test  = grid_clf.predict(X_test)\n",
    "\n",
    "    result['accuracy train']  = round(accuracy_score(y_train,y_pred_train),4)\n",
    "    result['accuracy test']   = round(accuracy_score(y_test,y_pred_test),4)\n",
    "    result['custom accuracy train'] = round(custom_accuracy(y_train,y_pred_train),4)\n",
    "    result['custom accuracy test']  = round(custom_accuracy(y_test,y_pred_test),4)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def gridsearch_all_models(models, X_train, X_test, y_train, y_test, cv=4, scoring='accuracy'):\n",
    "    \"\"\"\n",
    "    This function will perform a grisearch on a list of models, output the time taken, \n",
    "    and return a list of results dictionaries for each gridsearch model.\n",
    "    \n",
    "    INPUTS:\n",
    "    models = List of tuples in the form (name, model, param_grid)\n",
    "        name  = text name of the model\n",
    "        model = model of pipeline model\n",
    "        param_grid = parameters to be used for gridsearch\n",
    "    X_train, X_test, y_train, y_test = train test split of data(X) and target(y).\n",
    "    \n",
    "    RETURNS:\n",
    "    List of dictionaries containing gridsearch model, selected parameters, and accuracy scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"GRIDSEARCH AND SCORE ALL MODELS:\")\n",
    "    start = time.time()\n",
    "    results = []\n",
    "\n",
    "    for name, model, param_grid in models:\n",
    "        start_model = time.time()\n",
    "        print(\"  \", name, end='')\n",
    "        results.append(get_gridsearch_result(name=name,\n",
    "                                             estimator=model,\n",
    "                                             param_grid=param_grid,\n",
    "                                             X_train=X_train,\n",
    "                                             X_test=X_test, \n",
    "                                             y_train=y_train,\n",
    "                                             y_test=y_test,\n",
    "                                             cv=cv,\n",
    "                                             scoring=scoring))\n",
    "\n",
    "        end_model = time.time()\n",
    "        print(\":\\t time\", time.strftime('%H:%M:%S', time.gmtime(end_model-start_model)))\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"TOTAL TIME:\", time.strftime('%H:%M:%S', time.gmtime(end-start)))\n",
    "    return results\n",
    "\n",
    "\n",
    "def test_the_classifier(model, review_text):\n",
    "    \"\"\"\n",
    "    This function is used for amusement purposes to quickly get a prediction using the provided review text and model.\n",
    "    \"\"\"\n",
    "    stopwords = stopwords.words('english')\n",
    "    tokenizer = TweetTokenizer(preserve_case=False).tokenize\n",
    "    temp_review = Review(review_text, 'dummy_date', 0)\n",
    "    temp_review.clean_text()\n",
    "    temp_review.tokenize(tokenizer)\n",
    "    temp_review.remove_stopwords(stopwords)\n",
    "    \n",
    "    return model.predict([temp_review.get_all_tokens()])[0]\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None,\n",
    "                          custom_stat_value=None,\n",
    "                          custom_stat_text=''):\n",
    "    '''\n",
    "    This function will make a pretty plot of an sklearn Confusion Matrix using a Seaborn heatmap visualization.\n",
    "    Arguments\n",
    "    ---------\n",
    "    cf:            confusion matrix to be passed in\n",
    "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
    "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
    "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
    "    normalize:     If True, show the proportions for each category. Default is True.\n",
    "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
    "                   Default is True.\n",
    "    xyticks:       If True, show x and y ticks. Default is True.\n",
    "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
    "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
    "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
    "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
    "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "    '''\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "        stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text += \"\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                precision,recall,f1_score)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "        \n",
    "    if custom_stat_value:\n",
    "        stats_text += \"\\n{}={:0.3f}\".format(custom_stat_text,custom_stat_value)\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
