{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "\n",
    "def load_GloVe_vectors(file, vocab):\n",
    "    \"\"\"\n",
    "    This function will load Global Vectors for words in vocab from the specified GloVe file.\n",
    "    \n",
    "    INPUT:\n",
    "    file  = The path/filename of the file containing GloVe information.\n",
    "    vocab = The list of words that will be loaded from the GloVe file.\n",
    "    \"\"\"\n",
    "    glove = {}\n",
    "    with open(file, 'rb') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0].decode('utf-8')\n",
    "            if word in vocab:\n",
    "                vector = np.array(parts[1:], dtype=np.float32)\n",
    "                glove[word] = vector\n",
    "    return glove\n",
    "\n",
    "\n",
    "#Creating Mean Word Embeddings using Mean Embedding Vectorizer class\n",
    "class W2vVectorizer(object):\n",
    "    \"\"\"\n",
    "    This class is used to provide mean word vectors for review documents. \n",
    "    This is done in the transform function which is used to generate mean vectors in model pipelines.\n",
    "    The class has both fit and transform functions so that it may be used in an sklearn Pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.w2v = model\n",
    "        \n",
    "        #If using GloVe the model is in a dictionary format\n",
    "        if isinstance(model, dict):\n",
    "            if len(model) == 0:\n",
    "                self.dimensions = 0\n",
    "            else:\n",
    "                self.dimensions = len(model[next(iter(model))])\n",
    "        #Otherwise, using gensim keyed vector\n",
    "        else:\n",
    "            self.dimensions = model.vector_size\n",
    "    \n",
    "    # Need to implement a fit method as required for sklearn Pipeline.\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        This function generates a w2v vector for a set of tokens. This is done by taking \n",
    "        the mean of each token in the review.\n",
    "        \"\"\"\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                    or [np.zeros(self.dimensions)], axis=0) for words in X])\n",
    "    \n",
    "class KerasTokenizer(object):\n",
    "    \"\"\"\n",
    "    This class is used to fit text and convert text to sequences for use in a Keras NN Model.\n",
    "    The class has both fit and transform functions so that it may be used in an sklearn Pipeline.\n",
    "    num_words = max number of words to keep.\n",
    "    maxlen  = max length of all sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_words=20000, maxlen=100):\n",
    "        self.tokenizer = text.Tokenizer(num_words=num_words)\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.tokenizer.fit_on_texts(X)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return sequence.pad_sequences(self.tokenizer.texts_to_sequences(X), maxlen=self.maxlen)\n",
    "    \n",
    "class KerasModel(object):\n",
    "    \"\"\"\n",
    "    This class is used to fit and transform a keras model for use in an sklearn Pipeline.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, epochs=3, batch_size=32, validation_split=0.1):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        \n",
    "    def set_params(self, epochs=3, batch_size=32, validation_split=0.1):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y_dummies = pd.get_dummies(y).values\n",
    "        self.labels = np.array(pd.get_dummies(y).columns)\n",
    "        self.model.fit(X, y_dummies, epochs=self.epochs, batch_size=self.batch_size, validation_split=self.validation_split)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.model.predict(X)\n",
    "        return [self.labels[idx] for idx in y_pred.argmax(axis=1)]\n",
    "    \n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "        \n",
    "    \n",
    "def custom_accuracy(y_true, y_pred, threshold=1, credit_given=0.5):\n",
    "    \"\"\"\n",
    "    If y_pred is off by threshold or less, give partial credit according to credit_given.\n",
    "    INPUTS:\n",
    "    y_true       = True label.\n",
    "    y_pred       = Predicted label.\n",
    "    threshold    = Threshold for giving credit to inaccurate predictions. (=1 gives credit to predictions off by 1)\n",
    "    credit_given = Partial credit amount for close but inaccurate predictions. (=0.5 give 50% credit)\n",
    "    \"\"\"\n",
    "    predicted_correct = sum((y_true-y_pred)==0)\n",
    "    predicted_off = sum(abs(y_true-y_pred)<=threshold) - predicted_correct\n",
    "    custom_accuracy = (predicted_correct + credit_given*predicted_off)/len(y_true)\n",
    "    return custom_accuracy\n",
    "\n",
    "\n",
    "def get_gridsearch_result(name, estimator, param_grid, X_train, X_test, y_train, y_test, cv=4, scoring='accuracy'):\n",
    "    \"\"\"\n",
    "    This function fits a GridSearchCV model and populates a dictionary containing the best model and accuracy results.\n",
    "    \n",
    "    INPUTS:\n",
    "    name       = The name of the gridsearch model desired. It will be used as a key in the returned dictionary object.\n",
    "    estimator  = The model which will be passed into GridSearchCV. It can be a pipeline model or a base model.\n",
    "    param_grid = The parameter grid to be used by GridSearchCV.\n",
    "    X_train, X_test, y_train, y_test = train test split of data(X) and target(y).\n",
    "    cv         = Number of cross validations to perform.\n",
    "    \n",
    "    RETURN:\n",
    "    Dictionary containing the fitted GridSearchCV model as well as summary metrics.\n",
    "    Dictionary keys are: name, model, model params, accuracy train, accuracy test, \n",
    "                         custom accuracy train, custom accuracy test.\n",
    "    \"\"\"\n",
    "    grid_clf = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=cv, scoring=scoring)\n",
    "    grid_clf.fit(X_train,y_train)\n",
    "\n",
    "    result = {}\n",
    "    \n",
    "    result['name'] = name\n",
    "    result['model'] = grid_clf.best_estimator_\n",
    "    result['model params'] = grid_clf.best_params_\n",
    "\n",
    "    y_pred_train = grid_clf.predict(X_train)\n",
    "    y_pred_test  = grid_clf.predict(X_test)\n",
    "\n",
    "    result['accuracy train']  = round(accuracy_score(y_train,y_pred_train),4)\n",
    "    result['accuracy test']   = round(accuracy_score(y_test,y_pred_test),4)\n",
    "    result['custom accuracy train'] = round(custom_accuracy(y_train,y_pred_train),4)\n",
    "    result['custom accuracy test']  = round(custom_accuracy(y_test,y_pred_test),4)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def gridsearch_all_models(models, X_train, X_test, y_train, y_test, cv=4, scoring='accuracy'):\n",
    "    \"\"\"\n",
    "    This function will perform a grisearch on a list of models, output the time taken, \n",
    "    and return a list of results dictionaries for each gridsearch model.\n",
    "    \n",
    "    INPUTS:\n",
    "    models = List of tuples in the form (name, model, param_grid)\n",
    "        name  = text name of the model\n",
    "        model = model of pipeline model\n",
    "        param_grid = parameters to be used for gridsearch\n",
    "    X_train, X_test, y_train, y_test = train test split of data(X) and target(y).\n",
    "    \n",
    "    RETURNS:\n",
    "    List of dictionaries containing gridsearch model, selected parameters, and accuracy scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"GRIDSEARCH AND SCORE ALL MODELS:\")\n",
    "    start = time.time()\n",
    "    results = []\n",
    "\n",
    "    for name, model, param_grid in models:\n",
    "        start_model = time.time()\n",
    "        print(\"  \", name, end='')\n",
    "        results.append(get_gridsearch_result(name=name,\n",
    "                                             estimator=model,\n",
    "                                             param_grid=param_grid,\n",
    "                                             X_train=X_train,\n",
    "                                             X_test=X_test, \n",
    "                                             y_train=y_train,\n",
    "                                             y_test=y_test,\n",
    "                                             cv=cv,\n",
    "                                             scoring=scoring))\n",
    "\n",
    "        end_model = time.time()\n",
    "        print(\":\\t time\", time.strftime('%H:%M:%S', time.gmtime(end_model-start_model)))\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"TOTAL TIME:\", time.strftime('%H:%M:%S', time.gmtime(end-start)))\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
