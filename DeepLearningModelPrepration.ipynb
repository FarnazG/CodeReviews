{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "\n",
    "def load_GloVe_vectors(file, vocab):\n",
    "    \"\"\"\n",
    "    This function will load Global Vectors for words in vocab from the specified GloVe file.\n",
    "    \n",
    "    INPUT:\n",
    "    file  = The path/filename of the file containing GloVe information.\n",
    "    vocab = The list of words that will be loaded from the GloVe file.\n",
    "    \"\"\"\n",
    "    glove = {}\n",
    "    with open(file, 'rb') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0].decode('utf-8')\n",
    "            if word in vocab:\n",
    "                vector = np.array(parts[1:], dtype=np.float32)\n",
    "                glove[word] = vector\n",
    "    return glove\n",
    "\n",
    "\n",
    "#Creating Mean Word Embeddings using Mean Embedding Vectorizer class\n",
    "class W2vVectorizer(object):\n",
    "    \"\"\"\n",
    "    This class is used to provide mean word vectors for review documents. \n",
    "    This is done in the transform function which is used to generate mean vectors in model pipelines.\n",
    "    The class has both fit and transform functions so that it may be used in an sklearn Pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.w2v = model\n",
    "        \n",
    "        #If using GloVe the model is in a dictionary format\n",
    "        if isinstance(model, dict):\n",
    "            if len(model) == 0:\n",
    "                self.dimensions = 0\n",
    "            else:\n",
    "                self.dimensions = len(model[next(iter(model))])\n",
    "        #Otherwise, using gensim keyed vector\n",
    "        else:\n",
    "            self.dimensions = model.vector_size\n",
    "    \n",
    "    # Need to implement a fit method as required for sklearn Pipeline.\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        This function generates a w2v vector for a set of tokens. This is done by taking \n",
    "        the mean of each token in the review.\n",
    "        \"\"\"\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                    or [np.zeros(self.dimensions)], axis=0) for words in X])\n",
    "    \n",
    "class KerasTokenizer(object):\n",
    "    \"\"\"\n",
    "    This class is used to fit text and convert text to sequences for use in a Keras NN Model.\n",
    "    The class has both fit and transform functions so that it may be used in an sklearn Pipeline.\n",
    "    num_words = max number of words to keep.\n",
    "    maxlen  = max length of all sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_words=20000, maxlen=100):\n",
    "        self.tokenizer = text.Tokenizer(num_words=num_words)\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.tokenizer.fit_on_texts(X)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return sequence.pad_sequences(self.tokenizer.texts_to_sequences(X), maxlen=self.maxlen)\n",
    "    \n",
    "class KerasModel(object):\n",
    "    \"\"\"\n",
    "    This class is used to fit and transform a keras model for use in an sklearn Pipeline.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, epochs=3, batch_size=32, validation_split=0.1):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        \n",
    "    def set_params(self, epochs=3, batch_size=32, validation_split=0.1):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = validation_split\n",
    "        return self\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y_dummies = pd.get_dummies(y).values\n",
    "        self.labels = np.array(pd.get_dummies(y).columns)\n",
    "        self.model.fit(X, y_dummies, epochs=self.epochs, batch_size=self.batch_size, validation_split=self.validation_split)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.model.predict(X)\n",
    "        return [self.labels[idx] for idx in y_pred.argmax(axis=1)]\n",
    "    \n",
    "    def summary(self):\n",
    "        self.model.summary()\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
