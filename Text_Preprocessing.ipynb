{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text_Preprocessing_Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\farnaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\farnaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\farnaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.summarization.textcleaner import split_sentences\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from textblob import TextBlob, Word, Blobber\n",
    "# to install textblob in your conda packages:\n",
    "# 1. go to anaconda prompt\n",
    "# 2. cd Anaconda3>Scripts>conda install -c conda-forge textblob\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "\n",
    "1. Tokenization\n",
    "2. Punctuation removal\n",
    "3. Removing Stop Words\n",
    "4. Stemming words\n",
    "5. Other preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GREAT Boutique, GREAT  service & GREAT  clothing line.....If your looking for unique,different and adorable dresses...this is the place to go. I found this boutique 2 years ago when i was walking back to my car after i had my hair done for an a very special  party and   wondering what to wear... and  then by accident I was in frontof the Kishas Studio What a great accident.......I got my perfect dress and I have been a happy customer since then:)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a sample text to check the cleaning process:\n",
    "text = 'GREAT Boutique, GREAT  service & GREAT  clothing line.....If your looking for unique,different and adorable dresses...this is the place to go. I found this boutique 2 years ago when i was walking back to my car after i had my hair done for an a very special  party and   wondering what to wear... and  then by accident I was in frontof the Kishas Studio What a great accident.......I got my perfect dress and I have been a happy customer since then:)'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(string.punctuation)\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great boutique great service  great clothing line. if your looking for uniquedifferent and adorable dresses. this is the place to go. i found this boutique  years ago when i was walking back to my car after i had my hair done for an a very special party and wondering what to wear. and then by accident i was in frontof the kishas studio what a great accident. i got my perfect dress and i have been a happy customer since then'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define a function to fully clean the text:\n",
    "def text_cleaning(text):\n",
    "    \"\"\"\n",
    "    This function cleans a block of text.\n",
    "    Input:text = the text to be cleaned.\n",
    "    Output: the text stripped of punctuation and made lowercase.\n",
    "    \"\"\"\n",
    "    # u'\\xa0' represents a non-breaking space in the text block that needs to be removed.\n",
    "    text = text.replace(u'\\xa0', u' ')\n",
    "    \n",
    "    #remove multiple fullstops and make a single fullstop\n",
    "    text = re.sub('\\.+', '. ', text)\n",
    "    \n",
    "    #the code line \"text = re.sub('\\.+', ' ', text)\" will remove the \".\" itself too    \n",
    "    #text = text.replace('...',' ')\n",
    "    #text= text.replace('..','. ')\n",
    "   \n",
    "    \n",
    "    #remove multiple spaces and make a single space.\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    #remove all tokens that are not alphabetic\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    #normalization\n",
    "    text= text.lower()\n",
    "    \n",
    "    #remove punctuations\n",
    "    #punctuation marks, add . if needs be\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>/?@#$%^&*_~'''\n",
    "    #traverse the given string and if any punctuation marks occur replace it with null \n",
    "    for i in text: \n",
    "        if i in punctuations: \n",
    "            text = text.replace(i, \"\") \n",
    "                    \n",
    "    return text\n",
    "\n",
    "    #if removing stopwords and stemming is required:\n",
    "    \n",
    "    #tokenize the text,split it into tokens/words\n",
    "    #tokens = word_tokenize(text)\n",
    "    \n",
    "    #remove stopwords\n",
    "    #tokens = [token for token in tokens if not token in stop_words]\n",
    "    \n",
    "    #stem words\n",
    "    #porter = PorterStemmer()\n",
    "    #stemmed = [porter.stem(token) for token in tokens] \n",
    "\n",
    "    #return the cleaned text in a sentence format, and normalize them all with lowercase method.\n",
    "    #cleaned_text=' '.join([''.join(token).lower() for token in tokens])\n",
    "\n",
    "    #return clened_text\n",
    "    \n",
    "clean_text= text_cleaning(text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great boutique great service  great clothing line. If your looking for uniquedifferent and adorable dresses. This is the place to go. I found this boutique  years ago when i was walking back to my car after i had my hair done for an a very special party and wondering what to wear. And then by accident i was in frontof the kishas studio what a great accident. I got my perfect dress and i have been a happy customer since then'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def organize_review(text):\n",
    "    \"\"\"\n",
    "    This method will return the review text in a sentence format with the beginning of each\n",
    "    sentence capitalized.\n",
    "    \"\"\"\n",
    "    text = split_sentences(text)\n",
    "    \n",
    "    #considering that sentence ends with period, apastrophe or other Separation punctuation marks,?,!: \n",
    "    return ' '.join([sentence.capitalize() for sentence in text])\n",
    "    #return ''.join([''.join(sentence).capitalize() for sentence in text])\n",
    "    \n",
    "organize_review(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G', 'R', 'E', 'A', 'T', ' ', 'B', 'o', 'u', 't', 'i', 'q', 'u', 'e', ',', ' ', 'G', 'R', 'E', 'A', 'T', ' ', ' ', 's', 'e', 'r', 'v', 'i', 'c', 'e', ' ', '&', ' ', 'G', 'R', 'E', 'A', 'T', ' ', ' ', 'c', 'l', 'o', 't', 'h', 'i', 'n', 'g', ' ', 'l', 'i', 'n', 'e', '.', '.', '.', '.', '.', 'I', 'f', ' ', 'y', 'o', 'u', 'r', ' ', 'l', 'o', 'o', 'k', 'i', 'n', 'g', ' ', 'f', 'o', 'r', ' ', 'u', 'n', 'i', 'q', 'u', 'e', ',', 'd', 'i', 'f', 'f', 'e', 'r', 'e', 'n', 't', ' ', 'a', 'n', 'd', ' ', 'a', 'd', 'o', 'r', 'a', 'b', 'l', 'e', ' ', 'd', 'r', 'e', 's', 's', 'e', 's', '.', '.', '.', 't', 'h', 'i', 's', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'p', 'l', 'a', 'c', 'e', ' ', 't', 'o', ' ', 'g', 'o', '.', ' ', 'I', ' ', 'f', 'o', 'u', 'n', 'd', ' ', 't', 'h', 'i', 's', ' ', 'b', 'o', 'u', 't', 'i', 'q', 'u', 'e', ' ', '2', ' ', 'y', 'e', 'a', 'r', 's', ' ', 'a', 'g', 'o', ' ', 'w', 'h', 'e', 'n', ' ', 'i', ' ', 'w', 'a', 's', ' ', 'w', 'a', 'l', 'k', 'i', 'n', 'g', ' ', 'b', 'a', 'c', 'k', ' ', 't', 'o', ' ', 'm', 'y', ' ', 'c', 'a', 'r', ' ', 'a', 'f', 't', 'e', 'r', ' ', 'i', ' ', 'h', 'a', 'd', ' ', 'm', 'y', ' ', 'h', 'a', 'i', 'r', ' ', 'd', 'o', 'n', 'e', ' ', 'f', 'o', 'r', ' ', 'a', 'n', ' ', 'a', ' ', 'v', 'e', 'r', 'y', ' ', 's', 'p', 'e', 'c', 'i', 'a', 'l', ' ', ' ', 'p', 'a', 'r', 't', 'y', ' ', 'a', 'n', 'd', ' ', ' ', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'i', 'n', 'g', ' ', 'w', 'h', 'a', 't', ' ', 't', 'o', ' ', 'w', 'e', 'a', 'r', '.', '.', '.', ' ', 'a', 'n', 'd', ' ', ' ', 't', 'h', 'e', 'n', ' ', 'b', 'y', ' ', 'a', 'c', 'c', 'i', 'd', 'e', 'n', 't', ' ', 'I', ' ', 'w', 'a', 's', ' ', 'i', 'n', ' ', 'f', 'r', 'o', 'n', 't', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'K', 'i', 's', 'h', 'a', 's', ' ', 'S', 't', 'u', 'd', 'i', 'o', ' ', 'W', 'h', 'a', 't', ' ', 'a', ' ', 'g', 'r', 'e', 'a', 't', ' ', 'a', 'c', 'c', 'i', 'd', 'e', 'n', 't', '.', '.', '.', '.', '.', '.', '.', 'I', ' ', 'g', 'o', 't', ' ', 'm', 'y', ' ', 'p', 'e', 'r', 'f', 'e', 'c', 't', ' ', 'd', 'r', 'e', 's', 's', ' ', 'a', 'n', 'd', ' ', 'I', ' ', 'h', 'a', 'v', 'e', ' ', 'b', 'e', 'e', 'n', ' ', 'a', ' ', 'h', 'a', 'p', 'p', 'y', ' ', 'c', 'u', 's', 't', 'o', 'm', 'e', 'r', ' ', 's', 'i', 'n', 'c', 'e', ' ', 't', 'h', 'e', 'n', ':', ')']\n"
     ]
    }
   ],
   "source": [
    "def get_all_tokens(text):\n",
    "    return [token for sentence in text for token in sentence]\n",
    "\n",
    "print(get_all_tokens(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GREAT Boutique , GREAT service & GREAT clothing line ... ..If looking unique , different adorable dresses ... place go . I found boutique 2 years ago walking back car hair done special party wondering wear ... accident I frontof Kishas Studio What great accident ... ... .I got perfect dress I happy customer since : )'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[' '.join([word for word in sent_tokenize(text) if word not in stopwords])]\n",
    "\n",
    "# vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "# [' '.join([word for word in sentence.split(sep=\" \") if word in vocab]) for sentence in text]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "        \n",
    "def remove_stopwords(text, stop_words):\n",
    "    \"\"\"\n",
    "    This method removes stopwords from the review text.\n",
    "    INPUT: stopwords = List of stopwords to be removed.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    #remove stopwords\n",
    "    tokens = [token for token in tokens if not token in stop_words]\n",
    "    #return the cleaned text in a sentence format.\n",
    "    return ' '.join([''.join(token) for token in tokens])\n",
    "\n",
    "remove_stopwords(text, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great boutique great service  great clothing line. if your looking for uniquedifferent and adorable dresses. this is the place to go. i found this boutique  years ago when i was walking back to my car after i had my hair done for an a very special party and wondering what to wear. and then by accident i was in frontof the kishas studio what a great accident. i got my perfect dress and i have been a happy customer since then\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.6626984126984127, subjectivity=0.7492063492063492)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the sentiment function and returns two properties - polarity and subjectivity.\n",
    "text_sentiment= TextBlob(clean_text)\n",
    "print (text_sentiment)\n",
    "text_sentiment.sentiment\n",
    "\n",
    "#sentiment = TextBlob(clean_text).sentiment\n",
    "#sentiment\n",
    "#polarity = TextBlob(clean_text).sentiment.polarity\n",
    "#polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Review Class\n",
    "'''\n",
    "Define real words according to nltk corpus english vocabulary.\n",
    "This function is used for the remove_gibberish() method in the Review class.\n",
    "'''\n",
    "vocab_en = set(w.lower() for w in nltk.corpus.words.words())\n",
    "\n",
    "\n",
    "'''\n",
    "Define punctuations according to nltk corpus.\n",
    "This function is used for the text_cleaning method in the review class.\n",
    "'''\n",
    "punctuations = string.punctuation\n",
    "\n",
    "\n",
    "'''\n",
    "Define stopwords\n",
    "'''\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "\n",
    "'''\n",
    "Define sentence\n",
    "'''\n",
    "sentence = split_sentences(text)\n",
    "#or:sentence = sent_tokenize(text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
