{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text_Preprocessing_Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\farnaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\farnaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\farnaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.summarization.textcleaner import split_sentences\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from textblob import TextBlob, Word, Blobber\n",
    "# to install textblob in your conda packages:\n",
    "# 1. go to anaconda prompt\n",
    "# 2. cd Anaconda3>Scripts>conda install -c conda-forge textblob\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "\n",
    "1. Tokenization\n",
    "2. Punctuation removal\n",
    "3. Removing Stop Words\n",
    "4. Stemming words\n",
    "5. Other preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GREAT Boutique, GREAT  service & GREAT  clothing line.....If your looking for unique,different and adorable dresses...this is the place to go. I found this boutique 2 years ago when i was walking back to my car after i had my hair done for an a very special  party and   wondering what to wear... and  then by accident I was in frontof the Kishas Studio What a great accident.......I got my perfect dress and I have been a happy customer since then:)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a sample text to check the cleaning process:\n",
    "text = 'GREAT Boutique, GREAT  service & GREAT  clothing line.....If your looking for unique,different and adorable dresses...this is the place to go. I found this boutique 2 years ago when i was walking back to my car after i had my hair done for an a very special  party and   wondering what to wear... and  then by accident I was in frontof the Kishas Studio What a great accident.......I got my perfect dress and I have been a happy customer since then:)'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(string.punctuation)\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great boutique great service  great clothing line. if your looking for uniquedifferent and adorable dresses. this is the place to go. i found this boutique  years ago when i was walking back to my car after i had my hair done for an a very special party and wondering what to wear. and then by accident i was in frontof the kishas studio what a great accident. i got my perfect dress and i have been a happy customer since then'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define a function to fully clean the text:\n",
    "def text_cleaning(text):\n",
    "    \"\"\"\n",
    "    This function cleans a block of text.\n",
    "    Input:text = the text to be cleaned.\n",
    "    Output: the text stripped of punctuation and made lowercase.\n",
    "    \"\"\"\n",
    "    # u'\\xa0' represents a non-breaking space in the text block that needs to be removed.\n",
    "    text = text.replace(u'\\xa0', u' ')\n",
    "    \n",
    "    #remove multiple fullstops and make a single fullstop\n",
    "    text = re.sub('\\.+', '. ', text)\n",
    "    \n",
    "    #the code line \"text = re.sub('\\.+', ' ', text)\" will remove the \".\" itself too    \n",
    "    #text = text.replace('...',' ')\n",
    "    #text= text.replace('..','. ')\n",
    "   \n",
    "    \n",
    "    #remove multiple spaces and make a single space.\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    #remove all tokens that are not alphabetic\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    #normalization\n",
    "    text= text.lower()\n",
    "    \n",
    "    #remove punctuations\n",
    "    #punctuation marks, add . if needs be\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>/?@#$%^&*_~'''\n",
    "    #traverse the given string and if any punctuation marks occur replace it with null \n",
    "    for i in text: \n",
    "        if i in punctuations: \n",
    "            text = text.replace(i, \"\") \n",
    "                    \n",
    "    return text\n",
    "\n",
    "    #if removing stopwords and stemming is required:\n",
    "    \n",
    "    #tokenize the text,split it into tokens/words\n",
    "    #tokens = word_tokenize(text)\n",
    "    \n",
    "    #remove stopwords\n",
    "    #tokens = [token for token in tokens if not token in stop_words]\n",
    "    \n",
    "    #stem words\n",
    "    #porter = PorterStemmer()\n",
    "    #stemmed = [porter.stem(token) for token in tokens] \n",
    "\n",
    "    #return the cleaned text in a sentence format, and normalize them all with lowercase method.\n",
    "    #cleaned_text=' '.join([''.join(token).lower() for token in tokens])\n",
    "\n",
    "    #return clened_text\n",
    "    \n",
    "clean_text= text_cleaning(text)\n",
    "clean_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
